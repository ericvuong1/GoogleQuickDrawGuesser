{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "from keras.utils import to_categorical\n",
    "import math\n",
    "\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import cv2 as cv\n",
    "from PIL import Image\n",
    "\n",
    "import multiprocessing\n",
    "from multiprocessing.pool import ThreadPool\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "import keras\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "\n",
    "import cv2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/37\n",
      " - 10s - loss: 2.6068 - acc: 0.2649 - val_loss: 1.7400 - val_acc: 0.4980\n",
      "Epoch 2/37\n",
      " - 9s - loss: 1.7892 - acc: 0.4958 - val_loss: 1.2604 - val_acc: 0.6520\n",
      "Epoch 3/37\n",
      " - 8s - loss: 1.4891 - acc: 0.5797 - val_loss: 1.1972 - val_acc: 0.6700\n",
      "Epoch 4/37\n",
      " - 9s - loss: 1.2900 - acc: 0.6372 - val_loss: 1.0207 - val_acc: 0.7060\n",
      "Epoch 5/37\n",
      " - 8s - loss: 1.1694 - acc: 0.6628 - val_loss: 0.9669 - val_acc: 0.7120\n",
      "Epoch 6/37\n",
      " - 8s - loss: 1.0708 - acc: 0.6919 - val_loss: 0.8989 - val_acc: 0.7450\n",
      "Epoch 7/37\n",
      " - 8s - loss: 0.9914 - acc: 0.7172 - val_loss: 0.9476 - val_acc: 0.7430\n",
      "Epoch 8/37\n",
      " - 9s - loss: 0.9627 - acc: 0.7247 - val_loss: 0.8475 - val_acc: 0.7600\n",
      "Epoch 9/37\n",
      " - 9s - loss: 0.9110 - acc: 0.7420 - val_loss: 0.8154 - val_acc: 0.7680\n",
      "Epoch 10/37\n",
      " - 9s - loss: 0.8415 - acc: 0.7561 - val_loss: 0.8116 - val_acc: 0.7820\n",
      "Epoch 11/37\n",
      " - 8s - loss: 0.8024 - acc: 0.7634 - val_loss: 0.7593 - val_acc: 0.7860\n",
      "Epoch 12/37\n",
      " - 9s - loss: 0.7781 - acc: 0.7720 - val_loss: 0.8012 - val_acc: 0.7800\n",
      "Epoch 13/37\n",
      " - 9s - loss: 0.7498 - acc: 0.7781 - val_loss: 0.8098 - val_acc: 0.7850\n",
      "Epoch 14/37\n",
      " - 9s - loss: 0.7079 - acc: 0.7899 - val_loss: 0.7685 - val_acc: 0.7840\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 15/37\n",
      " - 8s - loss: 0.6231 - acc: 0.8126 - val_loss: 0.7540 - val_acc: 0.8030\n",
      "Epoch 16/37\n",
      " - 8s - loss: 0.5839 - acc: 0.8272 - val_loss: 0.7561 - val_acc: 0.8070\n",
      "Epoch 17/37\n",
      " - 8s - loss: 0.5499 - acc: 0.8327 - val_loss: 0.7601 - val_acc: 0.7980\n",
      "Epoch 18/37\n",
      " - 9s - loss: 0.5393 - acc: 0.8386 - val_loss: 0.7636 - val_acc: 0.7990\n",
      "Epoch 19/37\n",
      " - 9s - loss: 0.5246 - acc: 0.8361 - val_loss: 0.7905 - val_acc: 0.7940\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 20/37\n",
      " - 9s - loss: 0.4753 - acc: 0.8528 - val_loss: 0.7433 - val_acc: 0.7890\n",
      "Epoch 21/37\n",
      " - 8s - loss: 0.4503 - acc: 0.8666 - val_loss: 0.7249 - val_acc: 0.7940\n",
      "Epoch 22/37\n",
      " - 8s - loss: 0.4439 - acc: 0.8597 - val_loss: 0.7585 - val_acc: 0.7980\n",
      "Epoch 23/37\n",
      " - 8s - loss: 0.4235 - acc: 0.8681 - val_loss: 0.7550 - val_acc: 0.8050\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 24/37\n",
      " - 9s - loss: 0.3976 - acc: 0.8769 - val_loss: 0.7453 - val_acc: 0.8050\n",
      "Epoch 25/37\n",
      " - 9s - loss: 0.3945 - acc: 0.8763 - val_loss: 0.7535 - val_acc: 0.7970\n",
      "Epoch 26/37\n",
      " - 9s - loss: 0.3870 - acc: 0.8834 - val_loss: 0.7553 - val_acc: 0.8110\n",
      "Epoch 27/37\n",
      " - 8s - loss: 0.3828 - acc: 0.8819 - val_loss: 0.7616 - val_acc: 0.8030\n",
      "Epoch 28/37\n",
      " - 9s - loss: 0.3617 - acc: 0.8862 - val_loss: 0.7696 - val_acc: 0.8040\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 29/37\n",
      " - 9s - loss: 0.3586 - acc: 0.8898 - val_loss: 0.7578 - val_acc: 0.8030\n",
      "Epoch 30/37\n",
      " - 9s - loss: 0.3525 - acc: 0.8879 - val_loss: 0.7788 - val_acc: 0.8070\n",
      "Epoch 31/37\n",
      " - 9s - loss: 0.3568 - acc: 0.8886 - val_loss: 0.7714 - val_acc: 0.8090\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 32/37\n",
      " - 9s - loss: 0.3451 - acc: 0.8921 - val_loss: 0.7689 - val_acc: 0.8020\n",
      "Epoch 33/37\n",
      " - 9s - loss: 0.3377 - acc: 0.8934 - val_loss: 0.7740 - val_acc: 0.8030\n",
      "Epoch 34/37\n",
      " - 9s - loss: 0.3376 - acc: 0.8940 - val_loss: 0.7648 - val_acc: 0.8020\n",
      "Epoch 35/37\n",
      " - 9s - loss: 0.3386 - acc: 0.8927 - val_loss: 0.7612 - val_acc: 0.7980\n",
      "Epoch 36/37\n",
      " - 9s - loss: 0.3264 - acc: 0.8968 - val_loss: 0.7780 - val_acc: 0.7940\n",
      "\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "Epoch 37/37\n",
      " - 9s - loss: 0.3260 - acc: 0.8976 - val_loss: 0.7734 - val_acc: 0.7960\n"
     ]
    }
   ],
   "source": [
    "data = np.load(\"all/train_images.npy\",encoding='bytes')\n",
    "\n",
    "x = []\n",
    "for image in data:\n",
    "    image = image[1].reshape(100,100)\n",
    "    x.append(image)\n",
    "\n",
    "x_pre = []\n",
    "for image in data:\n",
    "    image = image[1].reshape(100,100)\n",
    "    x_pre.append(image)\n",
    "\n",
    "\n",
    "\n",
    "labels = pd.read_csv(\"all/train_labels.csv\")\n",
    "y = []\n",
    "for i in range(len(labels)):\n",
    "    label = labels['Category'][i]\n",
    "    y.append(label)\n",
    "\n",
    "\n",
    "x_train, x_validation, y_train, y_validation = train_test_split(x, y, test_size = 0.1, random_state=30)\n",
    "\n",
    "x_train_backup = x_train\n",
    "x_validation_backup = x_validation\n",
    "\n",
    "x_train = np.array(x_train).reshape(len(x_train), 100, 100, 1).astype('float32') / 255\n",
    "x_validation = np.array(x_validation).reshape(len(x_validation), 100, 100, 1).astype('float32') / 255\n",
    "\n",
    "encoder = LabelBinarizer()\n",
    "\n",
    "y_train = encoder.fit_transform(y_train)\n",
    "y_validation = encoder.fit_transform(y_validation)\n",
    "\n",
    "y_train_decoded = encoder.inverse_transform(y_train)\n",
    "y_validation_decoded = encoder.inverse_transform(y_train)\n",
    "\n",
    "\n",
    "##Setup CNN\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters=20,\n",
    "                 kernel_size=(4, 4),\n",
    "                 padding='same',\n",
    "                 input_shape=(100, 100, 1),\n",
    "                 activation='relu'))\n",
    "model.add(Conv2D(filters=20,\n",
    "                 kernel_size=(4, 4),\n",
    "                 padding='same',\n",
    "                 activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(3, 3)))\n",
    "model.add(Conv2D(filters=80,\n",
    "                 kernel_size=(4, 4),\n",
    "                 padding='same',\n",
    "                 activation='relu'))\n",
    "model.add(Conv2D(filters=80,\n",
    "                 kernel_size=(3, 3),\n",
    "                 padding='same',\n",
    "                 activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(3, 3)))\n",
    "model.add(Conv2D(filters=80,\n",
    "                 kernel_size=(3, 3),\n",
    "                 padding='same',\n",
    "                 activation='relu'))\n",
    "model.add(Conv2D(filters=80,\n",
    "                 kernel_size=(3, 3),\n",
    "                 padding='same',\n",
    "                 activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=512))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(units=256))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(31, activation='softmax'))\n",
    "\n",
    "optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0) #adam\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "##Fit CNN model\n",
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "    samplewise_center=False,  # set each sample mean to 0\n",
    "    featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "    samplewise_std_normalization=False,  # divide each input by its std\n",
    "    zca_whitening=False,  # apply ZCA whitening\n",
    "    rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "    zoom_range=0.1,  # Randomly zoom image\n",
    "    width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "    height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "    horizontal_flip=False,  # randomly flip images\n",
    "    vertical_flip=False)  # randomly flip images\n",
    "datagen.fit(x_train)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc',\n",
    "                                            patience=3,\n",
    "                                            verbose=1,\n",
    "                                            factor=0.5,\n",
    "                                            min_lr=0.00001)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 37\n",
    "\n",
    "train_history = model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
    "                                    epochs=epochs, validation_data=(x_validation, y_validation),\n",
    "                                    verbose=2, steps_per_epoch=x_train.shape[0] // batch_size\n",
    "                                    , callbacks=[learning_rate_reduction])\n",
    "\n",
    "data_test = np.load(\"all/test_images.npy\",encoding='bytes')\n",
    "\n",
    "x_test = []\n",
    "for image in data_test:\n",
    "    image = image[1].reshape(100,100)\n",
    "    x_test.append(image)\n",
    "\n",
    "x_test = np.array(x_test).reshape(len(x_test), 100, 100, 1).astype('float32') / 255\n",
    "\n",
    "prediction = model.predict_classes(x_test)\n",
    "df = pd.DataFrame(prediction)\n",
    "df.index += 1\n",
    "df.index.name = 'Id'\n",
    "df.columns = ['Category']\n",
    "df.to_csv('cnn.csv', header=True)\n",
    "\n",
    "word_class = []\n",
    "for i in range(len(df)):\n",
    "    c = encoder.classes_[df['Category'][i+1]]\n",
    "    word_class.append(c)\n",
    "\n",
    "word_class = np.array(word_class)\n",
    "word_class = pd.DataFrame(word_class)\n",
    "word_class.columns = ['Category']\n",
    "word_class.to_csv('cnn.csv', header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/37\n",
      " - 8s - loss: 3.7875 - acc: 0.0579 - val_loss: 6.7417 - val_acc: 0.0420\n",
      "Epoch 2/37\n",
      " - 6s - loss: 3.3190 - acc: 0.0884 - val_loss: 3.5017 - val_acc: 0.0700\n",
      "Epoch 3/37\n",
      " - 6s - loss: 3.2008 - acc: 0.0911 - val_loss: 3.0078 - val_acc: 0.1190\n",
      "Epoch 4/37\n",
      " - 6s - loss: 2.8543 - acc: 0.1665 - val_loss: 2.7239 - val_acc: 0.2020\n",
      "Epoch 5/37\n",
      " - 6s - loss: 2.4871 - acc: 0.2665 - val_loss: 2.2990 - val_acc: 0.3210\n",
      "Epoch 6/37\n",
      " - 6s - loss: 2.1489 - acc: 0.3653 - val_loss: 2.0280 - val_acc: 0.3920\n",
      "Epoch 7/37\n",
      " - 6s - loss: 1.9243 - acc: 0.4364 - val_loss: 1.8474 - val_acc: 0.4570\n",
      "Epoch 8/37\n",
      " - 6s - loss: 1.7793 - acc: 0.4820 - val_loss: 1.6697 - val_acc: 0.5170\n",
      "Epoch 9/37\n",
      " - 7s - loss: 1.6640 - acc: 0.5191 - val_loss: 1.5765 - val_acc: 0.5290\n",
      "Epoch 10/37\n",
      " - 7s - loss: 1.5527 - acc: 0.5520 - val_loss: 1.5412 - val_acc: 0.5750\n",
      "Epoch 11/37\n",
      " - 6s - loss: 1.4740 - acc: 0.5741 - val_loss: 1.4686 - val_acc: 0.5800\n",
      "Epoch 12/37\n",
      " - 6s - loss: 1.3584 - acc: 0.6065 - val_loss: 1.3831 - val_acc: 0.6210\n",
      "Epoch 13/37\n",
      " - 6s - loss: 1.2866 - acc: 0.6283 - val_loss: 1.3145 - val_acc: 0.6380\n",
      "Epoch 14/37\n",
      " - 6s - loss: 1.2239 - acc: 0.6395 - val_loss: 1.2891 - val_acc: 0.6520\n",
      "Epoch 15/37\n",
      " - 6s - loss: 1.1493 - acc: 0.6598 - val_loss: 1.2719 - val_acc: 0.6600\n",
      "Epoch 16/37\n",
      " - 6s - loss: 1.1059 - acc: 0.6803 - val_loss: 1.2528 - val_acc: 0.6500\n",
      "Epoch 17/37\n",
      " - 6s - loss: 1.0775 - acc: 0.6856 - val_loss: 1.1964 - val_acc: 0.6780\n",
      "Epoch 18/37\n",
      " - 6s - loss: 1.0464 - acc: 0.6933 - val_loss: 1.1442 - val_acc: 0.6880\n",
      "Epoch 19/37\n",
      " - 6s - loss: 1.0153 - acc: 0.7008 - val_loss: 1.2792 - val_acc: 0.6520\n",
      "Epoch 20/37\n",
      " - 6s - loss: 0.9806 - acc: 0.7159 - val_loss: 1.1410 - val_acc: 0.6830\n",
      "Epoch 21/37\n",
      " - 6s - loss: 0.9363 - acc: 0.7284 - val_loss: 1.0838 - val_acc: 0.7130\n",
      "Epoch 22/37\n",
      " - 6s - loss: 0.9186 - acc: 0.7269 - val_loss: 1.0439 - val_acc: 0.7020\n",
      "Epoch 23/37\n",
      " - 6s - loss: 0.8891 - acc: 0.7411 - val_loss: 1.0705 - val_acc: 0.7180\n",
      "Epoch 24/37\n",
      " - 6s - loss: 0.8513 - acc: 0.7449 - val_loss: 1.2371 - val_acc: 0.6820\n",
      "Epoch 25/37\n",
      " - 6s - loss: 0.8335 - acc: 0.7559 - val_loss: 1.1803 - val_acc: 0.6790\n",
      "Epoch 26/37\n",
      " - 6s - loss: 0.8170 - acc: 0.7563 - val_loss: 1.1375 - val_acc: 0.6990\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 27/37\n",
      " - 6s - loss: 0.7289 - acc: 0.7820 - val_loss: 0.9511 - val_acc: 0.7340\n",
      "Epoch 28/37\n",
      " - 6s - loss: 0.6700 - acc: 0.8003 - val_loss: 0.9677 - val_acc: 0.7300\n",
      "Epoch 29/37\n",
      " - 6s - loss: 0.6405 - acc: 0.8035 - val_loss: 0.9856 - val_acc: 0.7360\n",
      "Epoch 30/37\n",
      " - 6s - loss: 0.6556 - acc: 0.8026 - val_loss: 1.0152 - val_acc: 0.7400\n",
      "Epoch 31/37\n",
      " - 6s - loss: 0.6157 - acc: 0.8131 - val_loss: 0.9975 - val_acc: 0.7380\n",
      "Epoch 32/37\n",
      " - 6s - loss: 0.6181 - acc: 0.8124 - val_loss: 1.0607 - val_acc: 0.7370\n",
      "Epoch 33/37\n",
      " - 6s - loss: 0.6056 - acc: 0.8146 - val_loss: 1.0026 - val_acc: 0.7460\n",
      "Epoch 34/37\n",
      " - 6s - loss: 0.6005 - acc: 0.8185 - val_loss: 1.0143 - val_acc: 0.7440\n",
      "Epoch 35/37\n",
      " - 6s - loss: 0.5647 - acc: 0.8252 - val_loss: 1.0215 - val_acc: 0.7310\n",
      "Epoch 36/37\n",
      " - 6s - loss: 0.5593 - acc: 0.8294 - val_loss: 1.0453 - val_acc: 0.7320\n",
      "\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 37/37\n",
      " - 6s - loss: 0.5220 - acc: 0.8350 - val_loss: 0.9849 - val_acc: 0.7480\n"
     ]
    }
   ],
   "source": [
    "data = np.load(\"all/train_images.npy\",encoding='bytes')\n",
    "\n",
    "x = []\n",
    "for image in data:\n",
    "    image = image[1].reshape(100,100)\n",
    "    x.append(image)\n",
    "\n",
    "x_pre = []\n",
    "for image in data:\n",
    "    image = image[1].reshape(100,100)\n",
    "    image = preprocess(image)\n",
    "    image = preProcessImage(image)\n",
    "    image = rmEmpty(image)\n",
    "    x_pre.append(image)\n",
    "\n",
    "\n",
    "\n",
    "labels = pd.read_csv(\"all/train_labels.csv\")\n",
    "y = []\n",
    "for i in range(len(labels)):\n",
    "    label = labels['Category'][i]\n",
    "    y.append(label)\n",
    "\n",
    "\n",
    "x_train, x_validation, y_train, y_validation = train_test_split(x, y, test_size = 0.1, random_state=30)\n",
    "\n",
    "x_train_backup = x_train\n",
    "x_validation_backup = x_validation\n",
    "\n",
    "x_train = np.array(x_train).reshape(len(x_train), 100, 100, 1).astype('float32') / 255\n",
    "x_validation = np.array(x_validation).reshape(len(x_validation), 100, 100, 1).astype('float32') / 255\n",
    "\n",
    "encoder = LabelBinarizer()\n",
    "\n",
    "y_train = encoder.fit_transform(y_train)\n",
    "y_validation = encoder.fit_transform(y_validation)\n",
    "\n",
    "y_train_decoded = encoder.inverse_transform(y_train)\n",
    "y_validation_decoded = encoder.inverse_transform(y_train)\n",
    "\n",
    "\n",
    "##Setup CNN\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters=20,\n",
    "                 kernel_size=(4, 4),\n",
    "                 padding='same',\n",
    "                 input_shape=(100, 100, 1),\n",
    "                 activation='relu'))\n",
    "model.add(Conv2D(filters=20,\n",
    "                 kernel_size=(4, 4),\n",
    "                 padding='same',\n",
    "                 activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(3, 3)))\n",
    "model.add(Conv2D(filters=80,\n",
    "                 kernel_size=(4, 4),\n",
    "                 padding='same',\n",
    "                 activation='relu'))\n",
    "model.add(Conv2D(filters=80,\n",
    "                 kernel_size=(3, 3),\n",
    "                 padding='same',\n",
    "                 activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(3, 3)))\n",
    "model.add(Conv2D(filters=80,\n",
    "                 kernel_size=(3, 3),\n",
    "                 padding='same',\n",
    "                 activation='relu'))\n",
    "model.add(Conv2D(filters=80,\n",
    "                 kernel_size=(3, 3),\n",
    "                 padding='same',\n",
    "                 activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(units=256))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(31, activation='softmax'))\n",
    "\n",
    "\n",
    "optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0) #adam\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "##Fit CNN model\n",
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "    samplewise_center=False,  # set each sample mean to 0\n",
    "    featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "    samplewise_std_normalization=False,  # divide each input by its std\n",
    "    zca_whitening=False,  # apply ZCA whitening\n",
    "    rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "    zoom_range=0.1,  # Randomly zoom image\n",
    "    width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "    height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "    horizontal_flip=False,  # randomly flip images\n",
    "    vertical_flip=False)  # randomly flip images\n",
    "datagen.fit(x_train)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc',\n",
    "                                            patience=3,\n",
    "                                            verbose=1,\n",
    "                                            factor=0.5,\n",
    "                                            min_lr=0.00001)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 37\n",
    "\n",
    "train_history = model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
    "                                    epochs=epochs, validation_data=(x_validation, y_validation),\n",
    "                                    verbose=2, steps_per_epoch=x_train.shape[0] // batch_size\n",
    "                                    , callbacks=[learning_rate_reduction])\n",
    "\n",
    "data_test = np.load(\"all/test_images.npy\",encoding='bytes')\n",
    "\n",
    "x_test = []\n",
    "for image in data_test:\n",
    "    image = image[1].reshape(100,100)\n",
    "    image = preprocess(image)\n",
    "    image = preProcessImage(image)\n",
    "    image = rmEmpty(image)\n",
    "    x_test.append(image)\n",
    "\n",
    "x_test = np.array(x_test).reshape(len(x_test), 100, 100, 1).astype('float32') / 255\n",
    "\n",
    "prediction = model.predict_classes(x_test)\n",
    "df = pd.DataFrame(prediction)\n",
    "df.index += 1\n",
    "df.index.name = 'Id'\n",
    "df.columns = ['Category']\n",
    "df.to_csv('cnn.csv', header=True)\n",
    "\n",
    "word_class = []\n",
    "for i in range(len(df)):\n",
    "    c = encoder.classes_[df['Category'][i+1]]\n",
    "    word_class.append(c)\n",
    "\n",
    "word_class = np.array(word_class)\n",
    "word_class = pd.DataFrame(word_class)\n",
    "word_class.columns = ['Category']\n",
    "word_class.to_csv('cnn.csv', header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(image, cutoff=127, maxContours=5):\n",
    "    image = np.uint8(image)\n",
    "    nb_components, output, stats, centroids = cv2.connectedComponentsWithStats(image,connectivity = 4)\n",
    "    sizes = stats[:,-1]\n",
    "    max_label = 1\n",
    "    max_size = sizes[1]\n",
    "    for i in range(2,nb_components):\n",
    "        if sizes[i] > max_size:\n",
    "            max_label = i\n",
    "            max_size = sizes[i]\n",
    "    img = np.zeros(output.shape)\n",
    "    img[output == max_label] = 255\n",
    "    return img\n",
    "\n",
    "def preProcessImage(image, cutoff=127, maxContours=10):\n",
    "    image = np.uint8(image)\n",
    "    im = np.uint8(image)\n",
    "    red, thresh = cv2.threshold(im, cutoff, 255, 0)\n",
    "    im2, contours, hierarchy= cv2.findContours(thresh, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    mask = np.zeros(im.shape, np.uint8)\n",
    "    largest_contours = sorted(contours, key=cv2.contourArea, reverse=True)\n",
    "    for ind, contour in enumerate(largest_contours[:5]):\n",
    "        x, y, w, h = cv2.boundingRect(contour)\n",
    "        mask[y:y+h, x:x+w] = 255    \n",
    "    filteredImage = cv2.bitwise_and(thresh, thresh, mask=mask)\n",
    "    return filteredImage.reshape((image.shape))\n",
    "\n",
    "def rmEmpty(img):\n",
    "    nb_components, output, stats, centroids = cv2.connectedComponentsWithStats(img, connectivity=8)\n",
    "    sizes = stats[1:, -1]; nb_components = nb_components - 1\n",
    "    min_size = 50\n",
    "    img2 = np.zeros((output.shape))\n",
    "    for i in range(0, nb_components):\n",
    "        if sizes[i] >= min_size:\n",
    "            img2[output == i + 1] = 255\n",
    "    return img2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/37\n",
      " - 8s - loss: 3.8259 - acc: 0.0555 - val_loss: 5.2294 - val_acc: 0.0500\n",
      "Epoch 2/37\n",
      " - 6s - loss: 3.3271 - acc: 0.0817 - val_loss: 3.6531 - val_acc: 0.0570\n",
      "Epoch 3/37\n",
      " - 6s - loss: 3.2067 - acc: 0.0942 - val_loss: 4.8093 - val_acc: 0.0540\n",
      "Epoch 4/37\n",
      " - 6s - loss: 3.0712 - acc: 0.1183 - val_loss: 3.3514 - val_acc: 0.1050\n",
      "Epoch 5/37\n",
      " - 6s - loss: 2.6790 - acc: 0.2046 - val_loss: 2.8103 - val_acc: 0.1680\n",
      "Epoch 6/37\n",
      " - 6s - loss: 2.3592 - acc: 0.2979 - val_loss: 2.1622 - val_acc: 0.3490\n",
      "Epoch 7/37\n",
      " - 6s - loss: 2.0736 - acc: 0.3848 - val_loss: 2.0721 - val_acc: 0.3800\n",
      "Epoch 8/37\n",
      " - 6s - loss: 1.8793 - acc: 0.4422 - val_loss: 1.7874 - val_acc: 0.4670\n",
      "Epoch 9/37\n",
      " - 7s - loss: 1.6925 - acc: 0.4959 - val_loss: 1.6909 - val_acc: 0.4850\n",
      "Epoch 10/37\n",
      " - 8s - loss: 1.5690 - acc: 0.5455 - val_loss: 1.4933 - val_acc: 0.5500\n",
      "Epoch 11/37\n",
      " - 6s - loss: 1.4770 - acc: 0.5674 - val_loss: 1.4112 - val_acc: 0.5880\n",
      "Epoch 12/37\n",
      " - 6s - loss: 1.3809 - acc: 0.5990 - val_loss: 1.3895 - val_acc: 0.6070\n",
      "Epoch 13/37\n",
      " - 6s - loss: 1.3189 - acc: 0.6144 - val_loss: 1.2926 - val_acc: 0.6170\n",
      "Epoch 14/37\n",
      " - 6s - loss: 1.2289 - acc: 0.6400 - val_loss: 1.3673 - val_acc: 0.5870\n",
      "Epoch 15/37\n",
      " - 7s - loss: 1.1969 - acc: 0.6515 - val_loss: 1.2984 - val_acc: 0.6430\n",
      "Epoch 16/37\n",
      " - 6s - loss: 1.1312 - acc: 0.6726 - val_loss: 1.2800 - val_acc: 0.6420\n",
      "Epoch 17/37\n",
      " - 6s - loss: 1.0949 - acc: 0.6796 - val_loss: 1.2646 - val_acc: 0.6590\n",
      "Epoch 18/37\n",
      " - 6s - loss: 1.0467 - acc: 0.6917 - val_loss: 1.2518 - val_acc: 0.6670\n",
      "Epoch 19/37\n",
      " - 6s - loss: 1.0383 - acc: 0.6951 - val_loss: 1.1922 - val_acc: 0.6900\n",
      "Epoch 20/37\n",
      " - 6s - loss: 1.0046 - acc: 0.7005 - val_loss: 1.2063 - val_acc: 0.6720\n",
      "Epoch 21/37\n",
      " - 6s - loss: 0.9680 - acc: 0.7169 - val_loss: 1.1771 - val_acc: 0.6770\n",
      "Epoch 22/37\n",
      " - 6s - loss: 0.9237 - acc: 0.7329 - val_loss: 1.0778 - val_acc: 0.7020\n",
      "Epoch 23/37\n",
      " - 6s - loss: 0.8866 - acc: 0.7402 - val_loss: 1.0996 - val_acc: 0.7010\n",
      "Epoch 24/37\n",
      " - 6s - loss: 0.8527 - acc: 0.7447 - val_loss: 1.0904 - val_acc: 0.7130\n",
      "Epoch 25/37\n",
      " - 6s - loss: 0.8225 - acc: 0.7548 - val_loss: 1.0002 - val_acc: 0.7240\n",
      "Epoch 26/37\n",
      " - 6s - loss: 0.8343 - acc: 0.7537 - val_loss: 1.0572 - val_acc: 0.7050\n",
      "Epoch 27/37\n",
      " - 6s - loss: 0.7877 - acc: 0.7609 - val_loss: 1.0670 - val_acc: 0.7060\n",
      "Epoch 28/37\n",
      " - 6s - loss: 0.7793 - acc: 0.7689 - val_loss: 1.0845 - val_acc: 0.7070\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 29/37\n",
      " - 6s - loss: 0.6568 - acc: 0.7989 - val_loss: 0.9895 - val_acc: 0.7480\n",
      "Epoch 30/37\n",
      " - 6s - loss: 0.6351 - acc: 0.8107 - val_loss: 0.9803 - val_acc: 0.7420\n",
      "Epoch 31/37\n",
      " - 6s - loss: 0.6115 - acc: 0.8162 - val_loss: 0.9916 - val_acc: 0.7480\n",
      "Epoch 32/37\n",
      " - 6s - loss: 0.5831 - acc: 0.8232 - val_loss: 0.9954 - val_acc: 0.7380\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 33/37\n",
      " - 6s - loss: 0.5494 - acc: 0.8335 - val_loss: 0.9615 - val_acc: 0.7490\n",
      "Epoch 34/37\n",
      " - 6s - loss: 0.5255 - acc: 0.8406 - val_loss: 0.9497 - val_acc: 0.7590\n",
      "Epoch 35/37\n",
      " - 6s - loss: 0.5116 - acc: 0.8411 - val_loss: 0.9910 - val_acc: 0.7460\n",
      "Epoch 36/37\n",
      " - 6s - loss: 0.5068 - acc: 0.8443 - val_loss: 0.9779 - val_acc: 0.7480\n",
      "Epoch 37/37\n",
      " - 6s - loss: 0.5065 - acc: 0.8441 - val_loss: 0.9377 - val_acc: 0.7610\n"
     ]
    }
   ],
   "source": [
    "data = np.load(\"all/train_images.npy\",encoding='bytes')\n",
    "\n",
    "x = []\n",
    "for image in data:\n",
    "    image = image[1].reshape(100,100)\n",
    "    x.append(image)\n",
    "\n",
    "x_pre = []\n",
    "for image in data:\n",
    "    image = image[1].reshape(100,100)\n",
    "#     image = preprocess(image)\n",
    "#     image = preProcessImage(image)\n",
    "#     image = rmEmpty(image)\n",
    "    x_pre.append(image)\n",
    "\n",
    "\n",
    "\n",
    "labels = pd.read_csv(\"all/train_labels.csv\")\n",
    "y = []\n",
    "for i in range(len(labels)):\n",
    "    label = labels['Category'][i]\n",
    "    y.append(label)\n",
    "\n",
    "\n",
    "x_train, x_validation, y_train, y_validation = train_test_split(x, y, test_size = 0.1, random_state=30)\n",
    "\n",
    "x_train_backup = x_train\n",
    "x_validation_backup = x_validation\n",
    "\n",
    "x_train = np.array(x_train).reshape(len(x_train), 100, 100, 1).astype('float32') / 255\n",
    "x_validation = np.array(x_validation).reshape(len(x_validation), 100, 100, 1).astype('float32') / 255\n",
    "\n",
    "encoder = LabelBinarizer()\n",
    "\n",
    "y_train = encoder.fit_transform(y_train)\n",
    "y_validation = encoder.fit_transform(y_validation)\n",
    "\n",
    "y_train_decoded = encoder.inverse_transform(y_train)\n",
    "y_validation_decoded = encoder.inverse_transform(y_train)\n",
    "\n",
    "\n",
    "##Setup CNN\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters=20,\n",
    "                 kernel_size=(4, 4),\n",
    "                 padding='same',\n",
    "                 input_shape=(100, 100, 1),\n",
    "                 activation='relu'))\n",
    "model.add(Conv2D(filters=20,\n",
    "                 kernel_size=(4, 4),\n",
    "                 padding='same',\n",
    "                 activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(3, 3)))\n",
    "model.add(Conv2D(filters=80,\n",
    "                 kernel_size=(4, 4),\n",
    "                 padding='same',\n",
    "                 activation='relu'))\n",
    "model.add(Conv2D(filters=80,\n",
    "                 kernel_size=(3, 3),\n",
    "                 padding='same',\n",
    "                 activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(3, 3)))\n",
    "model.add(Conv2D(filters=80,\n",
    "                 kernel_size=(3, 3),\n",
    "                 padding='same',\n",
    "                 activation='relu'))\n",
    "model.add(Conv2D(filters=80,\n",
    "                 kernel_size=(3, 3),\n",
    "                 padding='same',\n",
    "                 activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(units=256))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(31, activation='softmax'))\n",
    "\n",
    "\n",
    "optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0) #adam\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "##Fit CNN model\n",
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "    samplewise_center=False,  # set each sample mean to 0\n",
    "    featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "    samplewise_std_normalization=False,  # divide each input by its std\n",
    "    zca_whitening=False,  # apply ZCA whitening\n",
    "    rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "    zoom_range=0.1,  # Randomly zoom image\n",
    "    width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "    height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "    horizontal_flip=False,  # randomly flip images\n",
    "    vertical_flip=False)  # randomly flip images\n",
    "datagen.fit(x_train)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc',\n",
    "                                            patience=3,\n",
    "                                            verbose=1,\n",
    "                                            factor=0.5,\n",
    "                                            min_lr=0.00001)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 37\n",
    "\n",
    "train_history = model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
    "                                    epochs=epochs, validation_data=(x_validation, y_validation),\n",
    "                                    verbose=2, steps_per_epoch=x_train.shape[0] // batch_size\n",
    "                                    , callbacks=[learning_rate_reduction])\n",
    "\n",
    "data_test = np.load(\"all/test_images.npy\",encoding='bytes')\n",
    "\n",
    "x_test = []\n",
    "for image in data_test:\n",
    "    image = image[1].reshape(100,100)\n",
    "#     image = preprocess(image)\n",
    "#     image = preProcessImage(image)\n",
    "#     image = rmEmpty(image)\n",
    "    x_test.append(image)\n",
    "\n",
    "x_test = np.array(x_test).reshape(len(x_test), 100, 100, 1).astype('float32') / 255\n",
    "\n",
    "prediction = model.predict_classes(x_test)\n",
    "df = pd.DataFrame(prediction)\n",
    "df.index += 1\n",
    "df.index.name = 'Id'\n",
    "df.columns = ['Category']\n",
    "df.to_csv('cnn.csv', header=True)\n",
    "\n",
    "word_class = []\n",
    "for i in range(len(df)):\n",
    "    c = encoder.classes_[df['Category'][i+1]]\n",
    "    word_class.append(c)\n",
    "\n",
    "word_class = np.array(word_class)\n",
    "word_class = pd.DataFrame(word_class)\n",
    "word_class.columns = ['Category']\n",
    "word_class.to_csv('cnn.csv', header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      " - 7s - loss: 3.8427 - acc: 0.0557 - val_loss: 5.5204 - val_acc: 0.0440\n",
      "Epoch 2/15\n",
      " - 6s - loss: 3.3264 - acc: 0.0830 - val_loss: 3.4085 - val_acc: 0.0850\n",
      "Epoch 3/15\n",
      " - 6s - loss: 3.2062 - acc: 0.0968 - val_loss: 3.2053 - val_acc: 0.0990\n",
      "Epoch 4/15\n",
      " - 6s - loss: 2.9120 - acc: 0.1642 - val_loss: 2.6212 - val_acc: 0.2330\n",
      "Epoch 5/15\n",
      " - 6s - loss: 2.5344 - acc: 0.2477 - val_loss: 2.6262 - val_acc: 0.2360\n",
      "Epoch 6/15\n",
      " - 6s - loss: 2.2538 - acc: 0.3198 - val_loss: 2.4727 - val_acc: 0.2960\n",
      "Epoch 7/15\n",
      " - 6s - loss: 2.0462 - acc: 0.3901 - val_loss: 2.2860 - val_acc: 0.3570\n",
      "Epoch 8/15\n",
      " - 6s - loss: 1.8388 - acc: 0.4529 - val_loss: 1.8078 - val_acc: 0.4680\n",
      "Epoch 9/15\n",
      " - 6s - loss: 1.7047 - acc: 0.5028 - val_loss: 1.6772 - val_acc: 0.5050\n",
      "Epoch 10/15\n",
      " - 6s - loss: 1.5478 - acc: 0.5503 - val_loss: 1.6751 - val_acc: 0.5100\n",
      "Epoch 11/15\n",
      " - 6s - loss: 1.4533 - acc: 0.5760 - val_loss: 1.5045 - val_acc: 0.5740\n",
      "Epoch 12/15\n",
      " - 6s - loss: 1.3487 - acc: 0.6157 - val_loss: 1.3379 - val_acc: 0.6160\n",
      "Epoch 13/15\n",
      " - 6s - loss: 1.2759 - acc: 0.6293 - val_loss: 1.2873 - val_acc: 0.6430\n",
      "Epoch 14/15\n",
      " - 6s - loss: 1.2052 - acc: 0.6509 - val_loss: 1.1982 - val_acc: 0.6690\n",
      "Epoch 15/15\n",
      " - 6s - loss: 1.1588 - acc: 0.6636 - val_loss: 1.1872 - val_acc: 0.6650\n"
     ]
    }
   ],
   "source": [
    "data = np.load(\"all/train_images.npy\",encoding='bytes')\n",
    "\n",
    "x = []\n",
    "for image in data:\n",
    "    image = image[1].reshape(100,100)\n",
    "    x.append(image)\n",
    "\n",
    "x_pre = []\n",
    "for image in data:\n",
    "    image = image[1].reshape(100,100)\n",
    "#     image = preprocess(image)\n",
    "#     image = preProcessImage(image)\n",
    "#     image = rmEmpty(image)\n",
    "    x_pre.append(image)\n",
    "\n",
    "\n",
    "\n",
    "labels = pd.read_csv(\"all/train_labels.csv\")\n",
    "y = []\n",
    "for i in range(len(labels)):\n",
    "    label = labels['Category'][i]\n",
    "    y.append(label)\n",
    "\n",
    "\n",
    "x_train, x_validation, y_train, y_validation = train_test_split(x, y, test_size = 0.1, random_state=30)\n",
    "\n",
    "x_train_backup = x_train\n",
    "x_validation_backup = x_validation\n",
    "\n",
    "x_train = np.array(x_train).reshape(len(x_train), 100, 100, 1).astype('float32') / 255\n",
    "x_validation = np.array(x_validation).reshape(len(x_validation), 100, 100, 1).astype('float32') / 255\n",
    "\n",
    "encoder = LabelBinarizer()\n",
    "\n",
    "y_train = encoder.fit_transform(y_train)\n",
    "y_validation = encoder.fit_transform(y_validation)\n",
    "\n",
    "y_train_decoded = encoder.inverse_transform(y_train)\n",
    "y_validation_decoded = encoder.inverse_transform(y_train)\n",
    "\n",
    "\n",
    "##Setup CNN\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters=20,\n",
    "                 kernel_size=(4, 4),\n",
    "                 padding='same',\n",
    "                 input_shape=(100, 100, 1),\n",
    "                 activation='relu'))\n",
    "model.add(Conv2D(filters=20,\n",
    "                 kernel_size=(4, 4),\n",
    "                 padding='same',\n",
    "                 activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(3, 3)))\n",
    "model.add(Conv2D(filters=80,\n",
    "                 kernel_size=(4, 4),\n",
    "                 padding='same',\n",
    "                 activation='relu'))\n",
    "model.add(Conv2D(filters=80,\n",
    "                 kernel_size=(3, 3),\n",
    "                 padding='same',\n",
    "                 activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(3, 3)))\n",
    "model.add(Conv2D(filters=80,\n",
    "                 kernel_size=(3, 3),\n",
    "                 padding='same',\n",
    "                 activation='relu'))\n",
    "model.add(Conv2D(filters=80,\n",
    "                 kernel_size=(3, 3),\n",
    "                 padding='same',\n",
    "                 activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(units=256))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(31, activation='softmax'))\n",
    "\n",
    "\n",
    "optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0) #adam\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "##Fit CNN model\n",
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "    samplewise_center=False,  # set each sample mean to 0\n",
    "    featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "    samplewise_std_normalization=False,  # divide each input by its std\n",
    "    zca_whitening=False,  # apply ZCA whitening\n",
    "    rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "    zoom_range=0.1,  # Randomly zoom image\n",
    "    width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "    height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "    horizontal_flip=False,  # randomly flip images\n",
    "    vertical_flip=False)  # randomly flip images\n",
    "datagen.fit(x_train)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc',\n",
    "                                            patience=3,\n",
    "                                            verbose=1,\n",
    "                                            factor=0.5,\n",
    "                                            min_lr=0.00001)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "batch_size = 40\n",
    "epochs = 15\n",
    "\n",
    "train_history = model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
    "                                    epochs=epochs, validation_data=(x_validation, y_validation),\n",
    "                                    verbose=2, steps_per_epoch=x_train.shape[0] // batch_size\n",
    "                                    , callbacks=[learning_rate_reduction])\n",
    "\n",
    "data_test = np.load(\"all/test_images.npy\",encoding='bytes')\n",
    "\n",
    "x_test = []\n",
    "for image in data_test:\n",
    "    image = image[1].reshape(100,100)\n",
    "#     image = preprocess(image)\n",
    "#     image = preProcessImage(image)\n",
    "#     image = rmEmpty(image)\n",
    "    x_test.append(image)\n",
    "\n",
    "x_test = np.array(x_test).reshape(len(x_test), 100, 100, 1).astype('float32') / 255\n",
    "\n",
    "prediction = model.predict_classes(x_test)\n",
    "df = pd.DataFrame(prediction)\n",
    "df.index += 1\n",
    "df.index.name = 'Id'\n",
    "df.columns = ['Category']\n",
    "df.to_csv('cnn.csv', header=True)\n",
    "\n",
    "word_class = []\n",
    "for i in range(len(df)):\n",
    "    c = encoder.classes_[df['Category'][i+1]]\n",
    "    word_class.append(c)\n",
    "\n",
    "word_class = np.array(word_class)\n",
    "word_class = pd.DataFrame(word_class)\n",
    "word_class.columns = ['Category']\n",
    "word_class.to_csv('cnn.csv', header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      " - 8s - loss: 3.9393 - acc: 0.0421 - val_loss: 4.1351 - val_acc: 0.0350\n",
      "Epoch 2/30\n",
      " - 6s - loss: 3.4323 - acc: 0.0742 - val_loss: 3.2709 - val_acc: 0.0660\n",
      "Epoch 3/30\n",
      " - 6s - loss: 3.2665 - acc: 0.0887 - val_loss: 3.0933 - val_acc: 0.1010\n",
      "Epoch 4/30\n",
      " - 6s - loss: 3.1635 - acc: 0.0967 - val_loss: 3.0031 - val_acc: 0.1340\n",
      "Epoch 5/30\n",
      " - 6s - loss: 3.0689 - acc: 0.1141 - val_loss: 2.9350 - val_acc: 0.1220\n",
      "Epoch 6/30\n",
      " - 6s - loss: 3.0005 - acc: 0.1213 - val_loss: 2.9752 - val_acc: 0.1410\n",
      "Epoch 7/30\n",
      " - 6s - loss: 2.9103 - acc: 0.1456 - val_loss: 2.8058 - val_acc: 0.1640\n",
      "Epoch 8/30\n",
      " - 6s - loss: 2.6639 - acc: 0.2079 - val_loss: 2.7196 - val_acc: 0.1790\n",
      "Epoch 9/30\n",
      " - 6s - loss: 2.4740 - acc: 0.2614 - val_loss: 2.2975 - val_acc: 0.3070\n",
      "Epoch 10/30\n",
      " - 6s - loss: 2.3184 - acc: 0.3019 - val_loss: 2.1525 - val_acc: 0.3660\n",
      "Epoch 11/30\n",
      " - 6s - loss: 2.1642 - acc: 0.3556 - val_loss: 2.1129 - val_acc: 0.3450\n",
      "Epoch 12/30\n",
      " - 6s - loss: 2.0267 - acc: 0.3814 - val_loss: 1.9331 - val_acc: 0.4030\n",
      "Epoch 13/30\n",
      " - 6s - loss: 1.9107 - acc: 0.4337 - val_loss: 1.6701 - val_acc: 0.5050\n",
      "Epoch 14/30\n",
      " - 6s - loss: 1.8099 - acc: 0.4669 - val_loss: 1.6011 - val_acc: 0.5310\n",
      "Epoch 15/30\n",
      " - 6s - loss: 1.7177 - acc: 0.4981 - val_loss: 1.5927 - val_acc: 0.5060\n",
      "Epoch 16/30\n",
      " - 6s - loss: 1.6541 - acc: 0.5141 - val_loss: 1.5522 - val_acc: 0.5550\n",
      "Epoch 17/30\n",
      " - 6s - loss: 1.5636 - acc: 0.5449 - val_loss: 1.4659 - val_acc: 0.5930\n",
      "Epoch 18/30\n",
      " - 6s - loss: 1.4955 - acc: 0.5621 - val_loss: 1.3919 - val_acc: 0.6090\n",
      "Epoch 19/30\n",
      " - 6s - loss: 1.4262 - acc: 0.5851 - val_loss: 1.4196 - val_acc: 0.5870\n",
      "Epoch 20/30\n",
      " - 6s - loss: 1.3603 - acc: 0.6140 - val_loss: 1.3213 - val_acc: 0.6260\n",
      "Epoch 21/30\n",
      " - 6s - loss: 1.3492 - acc: 0.6161 - val_loss: 1.3393 - val_acc: 0.6120\n",
      "Epoch 22/30\n",
      " - 6s - loss: 1.2933 - acc: 0.6272 - val_loss: 1.1784 - val_acc: 0.6730\n",
      "Epoch 23/30\n",
      " - 6s - loss: 1.2390 - acc: 0.6446 - val_loss: 1.2150 - val_acc: 0.6620\n",
      "Epoch 24/30\n",
      " - 6s - loss: 1.2170 - acc: 0.6510 - val_loss: 1.5013 - val_acc: 0.6130\n",
      "Epoch 25/30\n",
      " - 6s - loss: 1.1871 - acc: 0.6627 - val_loss: 1.1645 - val_acc: 0.6780\n",
      "Epoch 26/30\n",
      " - 6s - loss: 1.1694 - acc: 0.6674 - val_loss: 1.1937 - val_acc: 0.6670\n",
      "Epoch 27/30\n",
      " - 6s - loss: 1.1434 - acc: 0.6724 - val_loss: 1.0611 - val_acc: 0.7090\n",
      "Epoch 28/30\n",
      " - 6s - loss: 1.0945 - acc: 0.6879 - val_loss: 1.1051 - val_acc: 0.6960\n",
      "Epoch 29/30\n",
      " - 6s - loss: 1.0638 - acc: 0.6972 - val_loss: 1.1640 - val_acc: 0.6810\n",
      "Epoch 30/30\n",
      " - 7s - loss: 1.0652 - acc: 0.6969 - val_loss: 1.1470 - val_acc: 0.6800\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n"
     ]
    }
   ],
   "source": [
    "data = np.load(\"all/train_images.npy\",encoding='bytes')\n",
    "\n",
    "x = []\n",
    "for image in data:\n",
    "    image = image[1].reshape(100,100)\n",
    "    x.append(image)\n",
    "\n",
    "x_pre = []\n",
    "for image in data:\n",
    "    image = image[1].reshape(100,100)\n",
    "#     image = preprocess(image)\n",
    "#     image = preProcessImage(image)\n",
    "#     image = rmEmpty(image)\n",
    "    x_pre.append(image)\n",
    "\n",
    "\n",
    "\n",
    "labels = pd.read_csv(\"all/train_labels.csv\")\n",
    "y = []\n",
    "for i in range(len(labels)):\n",
    "    label = labels['Category'][i]\n",
    "    y.append(label)\n",
    "\n",
    "\n",
    "x_train, x_validation, y_train, y_validation = train_test_split(x, y, test_size = 0.1, random_state=30)\n",
    "\n",
    "x_train_backup = x_train\n",
    "x_validation_backup = x_validation\n",
    "\n",
    "x_train = np.array(x_train).reshape(len(x_train), 100, 100, 1).astype('float32') / 255\n",
    "x_validation = np.array(x_validation).reshape(len(x_validation), 100, 100, 1).astype('float32') / 255\n",
    "\n",
    "encoder = LabelBinarizer()\n",
    "\n",
    "y_train = encoder.fit_transform(y_train)\n",
    "y_validation = encoder.fit_transform(y_validation)\n",
    "\n",
    "y_train_decoded = encoder.inverse_transform(y_train)\n",
    "y_validation_decoded = encoder.inverse_transform(y_train)\n",
    "\n",
    "\n",
    "##Setup CNN\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters=20,\n",
    "                 kernel_size=(4, 4),\n",
    "                 padding='same',\n",
    "                 input_shape=(100, 100, 1),\n",
    "                 activation='relu'))\n",
    "model.add(Conv2D(filters=20,\n",
    "                 kernel_size=(4, 4),\n",
    "                 padding='same',\n",
    "                 activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(3, 3)))\n",
    "model.add(Conv2D(filters=80,\n",
    "                 kernel_size=(4, 4),\n",
    "                 padding='same',\n",
    "                 activation='relu'))\n",
    "model.add(Conv2D(filters=80,\n",
    "                 kernel_size=(3, 3),\n",
    "                 padding='same',\n",
    "                 activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(3, 3)))\n",
    "model.add(Conv2D(filters=80,\n",
    "                 kernel_size=(3, 3),\n",
    "                 padding='same',\n",
    "                 activation='relu'))\n",
    "model.add(Conv2D(filters=80,\n",
    "                 kernel_size=(3, 3),\n",
    "                 padding='same',\n",
    "                 activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(filters=40,\n",
    "                 kernel_size=(3, 3),\n",
    "                 padding='same',\n",
    "                 activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(units=256))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(31, activation='softmax'))\n",
    "\n",
    "\n",
    "optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0) #adam\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "##Fit CNN model\n",
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "    samplewise_center=False,  # set each sample mean to 0\n",
    "    featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "    samplewise_std_normalization=False,  # divide each input by its std\n",
    "    zca_whitening=False,  # apply ZCA whitening\n",
    "    rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "    zoom_range=0.1,  # Randomly zoom image\n",
    "    width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "    height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "    horizontal_flip=False,  # randomly flip images\n",
    "    vertical_flip=False)  # randomly flip images\n",
    "datagen.fit(x_train)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc',\n",
    "                                            patience=3,\n",
    "                                            verbose=1,\n",
    "                                            factor=0.5,\n",
    "                                            min_lr=0.00001)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "batch_size = 30\n",
    "epochs = 30\n",
    "\n",
    "train_history = model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
    "                                    epochs=epochs, validation_data=(x_validation, y_validation),\n",
    "                                    verbose=2, steps_per_epoch=x_train.shape[0] // batch_size\n",
    "                                    , callbacks=[learning_rate_reduction])\n",
    "\n",
    "data_test = np.load(\"all/test_images.npy\",encoding='bytes')\n",
    "\n",
    "x_test = []\n",
    "for image in data_test:\n",
    "    image = image[1].reshape(100,100)\n",
    "#     image = preprocess(image)\n",
    "#     image = preProcessImage(image)\n",
    "#     image = rmEmpty(image)\n",
    "    x_test.append(image)\n",
    "\n",
    "x_test = np.array(x_test).reshape(len(x_test), 100, 100, 1).astype('float32') / 255\n",
    "\n",
    "prediction = model.predict_classes(x_test)\n",
    "df = pd.DataFrame(prediction)\n",
    "df.index += 1\n",
    "df.index.name = 'Id'\n",
    "df.columns = ['Category']\n",
    "df.to_csv('cnn.csv', header=True)\n",
    "\n",
    "word_class = []\n",
    "for i in range(len(df)):\n",
    "    c = encoder.classes_[df['Category'][i+1]]\n",
    "    word_class.append(c)\n",
    "\n",
    "word_class = np.array(word_class)\n",
    "word_class = pd.DataFrame(word_class)\n",
    "word_class.columns = ['Category']\n",
    "word_class.to_csv('cnn.csv', header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(\"all/train_images.npy\",encoding='bytes')\n",
    "\n",
    "x = []\n",
    "for image in data:\n",
    "    image = image[1].reshape(100,100)\n",
    "    x.append(image)\n",
    "\n",
    "x_pre = []\n",
    "for image in data:\n",
    "    image = image[1].reshape(100,100)\n",
    "    x_pre.append(image)\n",
    "\n",
    "\n",
    "\n",
    "labels = pd.read_csv(\"all/train_labels.csv\")\n",
    "y = []\n",
    "for i in range(len(labels)):\n",
    "    label = labels['Category'][i]\n",
    "    y.append(label)\n",
    "\n",
    "\n",
    "x_train, x_validation, y_train, y_validation = train_test_split(x, y, test_size = 0.1, random_state=30)\n",
    "\n",
    "x_train_backup = x_train\n",
    "x_validation_backup = x_validation\n",
    "\n",
    "x_train = np.array(x_train).reshape(len(x_train), 100, 100, 1).astype('float32') / 255\n",
    "x_validation = np.array(x_validation).reshape(len(x_validation), 100, 100, 1).astype('float32') / 255\n",
    "\n",
    "encoder = LabelBinarizer()\n",
    "\n",
    "y_train = encoder.fit_transform(y_train)\n",
    "y_validation = encoder.fit_transform(y_validation)\n",
    "\n",
    "y_train_decoded = encoder.inverse_transform(y_train)\n",
    "y_validation_decoded = encoder.inverse_transform(y_train)\n",
    "\n",
    "\n",
    "##Setup CNN\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters=20,\n",
    "                 kernel_size=(4, 4),\n",
    "                 padding='same',\n",
    "                 input_shape=(100, 100, 1),\n",
    "                 activation='relu'))\n",
    "model.add(Conv2D(filters=20,\n",
    "                 kernel_size=(4, 4),\n",
    "                 padding='same',\n",
    "                 activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(3, 3)))\n",
    "model.add(Conv2D(filters=80,\n",
    "                 kernel_size=(4, 4),\n",
    "                 padding='same',\n",
    "                 activation='relu'))\n",
    "model.add(Conv2D(filters=80,\n",
    "                 kernel_size=(3, 3),\n",
    "                 padding='same',\n",
    "                 activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(3, 3)))\n",
    "model.add(Conv2D(filters=80,\n",
    "                 kernel_size=(3, 3),\n",
    "                 padding='same',\n",
    "                 activation='relu'))\n",
    "model.add(Conv2D(filters=80,\n",
    "                 kernel_size=(3, 3),\n",
    "                 padding='same',\n",
    "                 activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=512))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(units=256))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(31, activation='softmax'))\n",
    "\n",
    "optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0) #adam\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "##Fit CNN model\n",
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "    samplewise_center=False,  # set each sample mean to 0\n",
    "    featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "    samplewise_std_normalization=False,  # divide each input by its std\n",
    "    zca_whitening=False,  # apply ZCA whitening\n",
    "    rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "    zoom_range=0.1,  # Randomly zoom image\n",
    "    width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "    height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "    horizontal_flip=False,  # randomly flip images\n",
    "    vertical_flip=False)  # randomly flip images\n",
    "datagen.fit(x_train)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc',\n",
    "                                            patience=3,\n",
    "                                            verbose=1,\n",
    "                                            factor=0.5,\n",
    "                                            min_lr=0.00001)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 37\n",
    "\n",
    "train_history = model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
    "                                    epochs=epochs, validation_data=(x_validation, y_validation),\n",
    "                                    verbose=2, steps_per_epoch=x_train.shape[0] // batch_size\n",
    "                                    , callbacks=[learning_rate_reduction])\n",
    "\n",
    "data_test = np.load(\"all/test_images.npy\",encoding='bytes')\n",
    "\n",
    "x_test = []\n",
    "for image in data_test:\n",
    "    image = image[1].reshape(100,100)\n",
    "    x_test.append(image)\n",
    "\n",
    "x_test = np.array(x_test).reshape(len(x_test), 100, 100, 1).astype('float32') / 255\n",
    "\n",
    "prediction = model.predict_classes(x_test)\n",
    "df = pd.DataFrame(prediction)\n",
    "df.index += 1\n",
    "df.index.name = 'Id'\n",
    "df.columns = ['Category']\n",
    "df.to_csv('cnn.csv', header=True)\n",
    "\n",
    "word_class = []\n",
    "for i in range(len(df)):\n",
    "    c = encoder.classes_[df['Category'][i+1]]\n",
    "    word_class.append(c)\n",
    "\n",
    "word_class = np.array(word_class)\n",
    "word_class = pd.DataFrame(word_class)\n",
    "word_class.columns = ['Category']\n",
    "word_class.to_csv('cnn.csv', header=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
