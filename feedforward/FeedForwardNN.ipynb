{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (3.4.3.18)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from opencv-python) (1.14.5)\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 18.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.utils import to_categorical\n",
    "import math\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "import multiprocessing\n",
    "from multiprocessing.pool import ThreadPool\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "# preprocessing image and files\n",
    "def preProcessImageObsolete(image, cutoff=127, maxContours=5):\n",
    "    image = np.uint8(image)\n",
    "    im = np.uint8(image)\n",
    "    red, thresh = cv2.threshold(im, cutoff, 255, 0)\n",
    "    im2, contours, hierarchy = cv2.findContours(thresh, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    mask = np.uint8(np.ones(im.shape))\n",
    "    largest_contours = sorted(contours, key=cv2.contourArea)\n",
    "    for ind, contour in enumerate(largest_contours[maxContours:]):\n",
    "        mask = cv2.drawContours(mask, [largest_contours[ind]], -1, 0, -1)\n",
    "\n",
    "    filteredImage = cv2.bitwise_and(thresh, thresh, mask=mask)\n",
    "    ## plt.imshow(filteredImage)\n",
    "    return filteredImage\n",
    "\n",
    "\n",
    "def preprocess(image, cutoff=127, maxContours=5):\n",
    "    image = np.uint8(image)\n",
    "    nb_components, output, stats, centroids = cv2.connectedComponentsWithStats(image, connectivity=4)\n",
    "    sizes = stats[:, -1]\n",
    "    max_label = 1\n",
    "    max_size = sizes[1]\n",
    "    for i in range(2, nb_components):\n",
    "        if sizes[i] > max_size:\n",
    "            max_label = i\n",
    "            max_size = sizes[i]\n",
    "    img = np.zeros(output.shape)\n",
    "    img[output == max_label] = 255\n",
    "    return img\n",
    "\n",
    "def preProcessImage(image, cutoff=127, maxContours=10):\n",
    "    image = np.uint8(image)\n",
    "    im = np.uint8(image)\n",
    "    red, thresh = cv2.threshold(im, cutoff, 255, 0)\n",
    "    im2, contours, hierarchy= cv2.findContours(thresh, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    mask = np.zeros(im.shape, np.uint8)\n",
    "    largest_contours = sorted(contours, key=cv2.contourArea, reverse=True)\n",
    "    for ind, contour in enumerate(largest_contours[:5]):\n",
    "        x, y, w, h = cv2.boundingRect(contour)\n",
    "        mask[y:y+h, x:x+w] = 255\n",
    "    filteredImage = cv2.bitwise_and(thresh, thresh, mask=mask)\n",
    "    #plt.imshow(filteredImage)\n",
    "    #plt.figure()\n",
    "    #plt.imshow(thresh)\n",
    "    #plt.figure()\n",
    "    #plt.imshow(mask)\n",
    "    return filteredImage.reshape((image.shape))\n",
    "\n",
    "data = np.load(\"dataset/train_images.npy\",encoding='latin1')\n",
    "\n",
    "x = []\n",
    "for image in data:\n",
    "    image = image[1].reshape(100,100)\n",
    "    image = preprocess(image)\n",
    "    x.append(image)\n",
    "\n",
    "x_pre = []\n",
    "for image in data:\n",
    "    image = image[1].reshape(100,100)\n",
    "    x_pre.append(image)\n",
    "\n",
    "\n",
    "labels = pd.read_csv(\"dataset/train_labels.csv\")\n",
    "y = []\n",
    "for i in range(len(labels)):\n",
    "    label = labels['Category'][i]\n",
    "    y.append(label)\n",
    "\n",
    "x_train, x_validation, y_train, y_validation = train_test_split(x, y, test_size = 0.1, random_state=30)\n",
    "\n",
    "x_train_backup = x_train\n",
    "x_validation_backup = x_validation\n",
    "\n",
    "x_train = np.array(x_train).reshape(len(x_train), 10000).astype('float32') / 255\n",
    "x_validation = np.array(x_validation).reshape(len(x_validation), 10000).astype('float32') / 255\n",
    "\n",
    "encoder = LabelBinarizer()\n",
    "\n",
    "y_train = encoder.fit_transform(y_train)\n",
    "\n",
    "y_validation = encoder.fit_transform(y_validation)\n",
    "\n",
    "y_train_decoded = encoder.inverse_transform(y_train)\n",
    "y_validation_decoded = encoder.inverse_transform(y_train)\n",
    "# end of preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "11.367683202749198\n",
      "Epoch: 2\n",
      "9.382560847161962\n",
      "Epoch: 3\n",
      "8.641405653622064\n",
      "Epoch: 4\n",
      "8.098023139016345\n",
      "Epoch: 5\n",
      "7.6067901403404345\n",
      "Epoch: 6\n",
      "7.326486296065044\n",
      "Epoch: 7\n",
      "7.149688376053312\n",
      "Epoch: 8\n",
      "6.949998620766291\n",
      "Epoch: 9\n",
      "6.799629095876736\n",
      "Epoch: 10\n",
      "6.648676619991193\n",
      "Epoch: 11\n",
      "6.506239919601174\n",
      "Epoch: 12\n",
      "6.392709797508456\n",
      "Epoch: 13\n",
      "6.280889320087324\n",
      "Epoch: 14\n",
      "6.189890200201854\n",
      "Epoch: 15\n",
      "6.095111605408041\n",
      "Epoch: 16\n",
      "6.027192920587905\n",
      "Epoch: 17\n",
      "5.9428453144823346\n",
      "Epoch: 18\n",
      "5.891252381080036\n",
      "Epoch: 19\n",
      "5.81924905917299\n",
      "Epoch: 20\n",
      "5.773015304779729\n",
      "Epoch: 21\n",
      "5.708595177080926\n",
      "Epoch: 22\n",
      "5.665706520885088\n",
      "Epoch: 23\n",
      "5.609285103951182\n",
      "Epoch: 24\n",
      "5.567596479000132\n",
      "Epoch: 25\n",
      "5.51805948321908\n",
      "Epoch: 26\n",
      "5.480550551473775\n",
      "Epoch: 27\n",
      "5.439399094179964\n",
      "Epoch: 28\n",
      "5.406700023578758\n",
      "Epoch: 29\n",
      "5.37092657499516\n",
      "Epoch: 30\n",
      "5.340050559540706\n",
      "Epoch: 31\n",
      "5.3074968575111905\n",
      "Epoch: 32\n",
      "5.278412367481717\n",
      "Epoch: 33\n",
      "5.249356804841531\n",
      "Epoch: 34\n",
      "5.222699131802298\n",
      "Epoch: 35\n",
      "5.196001611278746\n",
      "Epoch: 36\n",
      "5.170347777027688\n",
      "Epoch: 37\n",
      "5.144568284704756\n",
      "Epoch: 38\n",
      "5.119503917244613\n",
      "Epoch: 39\n",
      "5.094735301772145\n",
      "Epoch: 40\n",
      "5.07097368781733\n",
      "Epoch: 41\n",
      "5.048184472063173\n",
      "Epoch: 42\n",
      "5.026779429910037\n",
      "Epoch: 43\n",
      "5.006473485376678\n",
      "Epoch: 44\n",
      "4.987177245153677\n",
      "Epoch: 45\n",
      "4.96856449511825\n",
      "Epoch: 46\n",
      "4.950477266017164\n",
      "Epoch: 47\n",
      "4.932678135066925\n",
      "Epoch: 48\n",
      "4.91517531165054\n",
      "Epoch: 49\n",
      "4.898123083834654\n",
      "Epoch: 50\n",
      "4.88167558734657\n",
      "Epoch: 51\n",
      "4.865801505706018\n",
      "Epoch: 52\n",
      "4.850545173420799\n",
      "Epoch: 53\n",
      "4.836062371664388\n",
      "Epoch: 54\n",
      "4.82226224398023\n",
      "Epoch: 55\n",
      "4.808745641030449\n",
      "Epoch: 56\n",
      "4.79519233745066\n",
      "Epoch: 57\n",
      "4.781498964792027\n",
      "Epoch: 58\n",
      "4.76767934155909\n",
      "Epoch: 59\n",
      "4.753849330140055\n",
      "Epoch: 60\n",
      "4.740173605365028\n",
      "Epoch: 61\n",
      "4.726766873775449\n",
      "Epoch: 62\n",
      "4.713615542743284\n",
      "Epoch: 63\n",
      "4.700565539351574\n",
      "Epoch: 64\n",
      "4.687476935770004\n",
      "Epoch: 65\n",
      "4.674319133243153\n",
      "Epoch: 66\n",
      "4.66111195966833\n",
      "Epoch: 67\n",
      "4.64789714380424\n",
      "Epoch: 68\n",
      "4.634841914878931\n",
      "Epoch: 69\n",
      "4.6222095149089295\n",
      "Epoch: 70\n",
      "4.610106271594298\n",
      "Epoch: 71\n",
      "4.598419382908959\n",
      "Epoch: 72\n",
      "4.586971824584352\n",
      "Epoch: 73\n",
      "4.575598211667296\n",
      "Epoch: 74\n",
      "4.564138444607608\n",
      "Epoch: 75\n",
      "4.55262632024874\n",
      "Epoch: 76\n",
      "4.541291348298575\n",
      "Epoch: 77\n",
      "4.530305148870344\n",
      "Epoch: 78\n",
      "4.519738393278439\n",
      "Epoch: 79\n",
      "4.5096255764834\n",
      "Epoch: 80\n",
      "4.49993253791118\n",
      "Epoch: 81\n",
      "4.490485584472705\n",
      "Epoch: 82\n",
      "4.481047015971416\n",
      "Epoch: 83\n",
      "4.471511124685112\n",
      "Epoch: 84\n",
      "4.461932074514887\n",
      "Epoch: 85\n",
      "4.452461278935738\n",
      "Epoch: 86\n",
      "4.443290476864821\n",
      "Epoch: 87\n",
      "4.434517097147275\n",
      "Epoch: 88\n",
      "4.426041462452343\n",
      "Epoch: 89\n",
      "4.417643761921673\n",
      "Epoch: 90\n",
      "4.409122059502551\n",
      "Epoch: 91\n",
      "4.400406107148512\n",
      "Epoch: 92\n",
      "4.391563095486936\n",
      "Epoch: 93\n",
      "4.382641528951726\n",
      "Epoch: 94\n",
      "4.373600216805238\n",
      "Epoch: 95\n",
      "4.364403097926955\n",
      "Epoch: 96\n",
      "4.355065555879572\n",
      "Epoch: 97\n",
      "4.345634844866204\n",
      "Epoch: 98\n",
      "4.336042972951085\n",
      "Epoch: 99\n",
      "4.326143963063124\n",
      "Epoch: 100\n",
      "4.315846283390541\n",
      "Epoch: 101\n",
      "4.305152946976728\n",
      "Epoch: 102\n",
      "4.294142782228972\n",
      "Epoch: 103\n",
      "4.282930598104935\n",
      "Epoch: 104\n",
      "4.271653869877818\n",
      "Epoch: 105\n",
      "4.2603987588694245\n",
      "Epoch: 106\n",
      "4.249179703171994\n",
      "Epoch: 107\n",
      "4.238044748151125\n",
      "Epoch: 108\n",
      "4.227123660350562\n",
      "Epoch: 109\n",
      "4.216532214884018\n",
      "Epoch: 110\n",
      "4.206299344630422\n",
      "Epoch: 111\n",
      "4.196373758478499\n",
      "Epoch: 112\n",
      "4.186694035979053\n",
      "Epoch: 113\n",
      "4.177256163574876\n",
      "Epoch: 114\n",
      "4.168107034357798\n",
      "Epoch: 115\n",
      "4.159309217609423\n",
      "Epoch: 116\n",
      "4.150959728559591\n",
      "Epoch: 117\n",
      "4.143192394752208\n",
      "Epoch: 118\n",
      "4.1360657844429145\n",
      "Epoch: 119\n",
      "4.129545028916017\n",
      "Epoch: 120\n",
      "4.123548609230005\n",
      "Epoch: 121\n",
      "4.117987518012357\n",
      "Epoch: 122\n",
      "4.1127856363906234\n",
      "Epoch: 123\n",
      "4.107856895571592\n",
      "Epoch: 124\n",
      "4.103069155062762\n",
      "Epoch: 125\n",
      "4.098202618739368\n",
      "Epoch: 126\n",
      "4.093030294073407\n",
      "Epoch: 127\n",
      "4.087470746226722\n",
      "Epoch: 128\n",
      "4.081553092021694\n",
      "Epoch: 129\n",
      "4.075330439936994\n",
      "Epoch: 130\n",
      "4.068812113549827\n",
      "Epoch: 131\n",
      "4.061974083160643\n",
      "Epoch: 132\n",
      "4.054861315797692\n",
      "Epoch: 133\n",
      "4.047607502761472\n",
      "Epoch: 134\n",
      "4.040363177352055\n",
      "Epoch: 135\n",
      "4.03323043473581\n",
      "Epoch: 136\n",
      "4.02624600537016\n",
      "Epoch: 137\n",
      "4.019410035418073\n",
      "Epoch: 138\n",
      "4.012718126501767\n",
      "Epoch: 139\n",
      "4.006189083605737\n",
      "Epoch: 140\n",
      "3.9998761651087933\n",
      "Epoch: 141\n",
      "3.9938329446434624\n",
      "Epoch: 142\n",
      "3.988063941113305\n",
      "Epoch: 143\n",
      "3.9825224774034838\n",
      "Epoch: 144\n",
      "3.9771537654486933\n",
      "Epoch: 145\n",
      "3.971939148306964\n",
      "Epoch: 146\n",
      "3.9668902921630727\n",
      "Epoch: 147\n",
      "3.9620227695691725\n",
      "Epoch: 148\n",
      "3.9573211064064067\n",
      "Epoch: 149\n",
      "3.9527122994112633\n",
      "Epoch: 150\n",
      "3.948079121106026\n",
      "Epoch: 151\n",
      "3.9433013402429546\n",
      "Epoch: 152\n",
      "3.938285773597351\n",
      "Epoch: 153\n",
      "3.932966608125854\n",
      "Epoch: 154\n",
      "3.927307267192993\n",
      "Epoch: 155\n",
      "3.921325512018125\n",
      "Epoch: 156\n",
      "3.9151050204510813\n",
      "Epoch: 157\n",
      "3.9087639570841146\n",
      "Epoch: 158\n",
      "3.9024062650611384\n",
      "Epoch: 159\n",
      "3.8961078766857034\n",
      "Epoch: 160\n",
      "3.8899337551073097\n",
      "Epoch: 161\n",
      "3.883946944573155\n",
      "Epoch: 162\n",
      "3.8781872583079604\n",
      "Epoch: 163\n",
      "3.8726559213965053\n",
      "Epoch: 164\n",
      "3.8673297208528625\n",
      "Epoch: 165\n",
      "3.862175450818926\n",
      "Epoch: 166\n",
      "3.857160218056799\n",
      "Epoch: 167\n",
      "3.8522679175055057\n",
      "Epoch: 168\n",
      "3.847501533218256\n",
      "Epoch: 169\n",
      "3.842859740577255\n",
      "Epoch: 170\n",
      "3.8383193041740453\n",
      "Epoch: 171\n",
      "3.8338484092707366\n",
      "Epoch: 172\n",
      "3.8294310949589376\n",
      "Epoch: 173\n",
      "3.8250721632281532\n",
      "Epoch: 174\n",
      "3.820785154732621\n",
      "Epoch: 175\n",
      "3.816575993986215\n",
      "Epoch: 176\n",
      "3.8124296254215557\n",
      "Epoch: 177\n",
      "3.8083142295864025\n",
      "Epoch: 178\n",
      "3.8041973476375457\n",
      "Epoch: 179\n",
      "3.800059806038164\n",
      "Epoch: 180\n",
      "3.7959036354573996\n",
      "Epoch: 181\n",
      "3.7917513327681656\n",
      "Epoch: 182\n",
      "3.7876415790612454\n",
      "Epoch: 183\n",
      "3.783623849029771\n",
      "Epoch: 184\n",
      "3.779741827996675\n",
      "Epoch: 185\n",
      "3.776010344585434\n",
      "Epoch: 186\n",
      "3.772410321290279\n",
      "Epoch: 187\n",
      "3.768904664498073\n",
      "Epoch: 188\n",
      "3.7654575277021682\n",
      "Epoch: 189\n",
      "3.762042450167822\n",
      "Epoch: 190\n",
      "3.7586416884725695\n",
      "Epoch: 191\n",
      "3.7552488301360087\n",
      "Epoch: 192\n",
      "3.7518647308837916\n",
      "Epoch: 193\n",
      "3.748481957520147\n",
      "Epoch: 194\n",
      "3.745082753280718\n",
      "Epoch: 195\n",
      "3.7416558631601777\n",
      "Epoch: 196\n",
      "3.738203076294341\n",
      "Epoch: 197\n",
      "3.7347281218756687\n",
      "Epoch: 198\n",
      "3.731232818872837\n",
      "Epoch: 199\n",
      "3.727725005479058\n",
      "Epoch: 200\n",
      "3.7242206381424476\n",
      "Epoch: 201\n",
      "3.7207370724502016\n",
      "Epoch: 202\n",
      "3.7172880559993824\n",
      "Epoch: 203\n",
      "3.7138875003662752\n",
      "Epoch: 204\n",
      "3.7105578107962365\n",
      "Epoch: 205\n",
      "3.7073317969529476\n",
      "Epoch: 206\n",
      "3.7042468317680006\n",
      "Epoch: 207\n",
      "3.7013352925072733\n",
      "Epoch: 208\n",
      "3.698608568474903\n",
      "Epoch: 209\n",
      "3.6960441278757683\n",
      "Epoch: 210\n",
      "3.693594061407723\n",
      "Epoch: 211\n",
      "3.6912083976373737\n",
      "Epoch: 212\n",
      "3.688846898479803\n",
      "Epoch: 213\n",
      "3.68647981492377\n",
      "Epoch: 214\n",
      "3.6840866970891284\n",
      "Epoch: 215\n",
      "3.681654856670889\n",
      "Epoch: 216\n",
      "3.6791793960600723\n",
      "Epoch: 217\n",
      "3.67666505572912\n",
      "Epoch: 218\n",
      "3.6741272226313675\n",
      "Epoch: 219\n",
      "3.67158614149847\n",
      "Epoch: 220\n",
      "3.6690576039288945\n",
      "Epoch: 221\n",
      "3.666547892135234\n",
      "Epoch: 222\n",
      "3.6640523160159466\n",
      "Epoch: 223\n",
      "3.661554289064621\n",
      "Epoch: 224\n",
      "3.659027926789099\n",
      "Epoch: 225\n",
      "3.6564477783494076\n",
      "Epoch: 226\n",
      "3.6538015536671162\n",
      "Epoch: 227\n",
      "3.651097741047139\n",
      "Epoch: 228\n",
      "3.6483597146792444\n",
      "Epoch: 229\n",
      "3.6456111999054563\n",
      "Epoch: 230\n",
      "3.6428699491155188\n",
      "Epoch: 231\n",
      "3.640153543543964\n",
      "Epoch: 232\n",
      "3.6374836254365976\n",
      "Epoch: 233\n",
      "3.634880000545399\n",
      "Epoch: 234\n",
      "3.6323542767315287\n",
      "Epoch: 235\n",
      "3.629909244288786\n",
      "Epoch: 236\n",
      "3.6275398126269596\n",
      "Epoch: 237\n",
      "3.6252332264970852\n",
      "Epoch: 238\n",
      "3.6229724959395635\n",
      "Epoch: 239\n",
      "3.6207451024277755\n",
      "Epoch: 240\n",
      "3.618549226982952\n",
      "Epoch: 241\n",
      "3.6163904518578893\n",
      "Epoch: 242\n",
      "3.6142737889937258\n",
      "Epoch: 243\n",
      "3.6122012529668246\n",
      "Epoch: 244\n",
      "3.610172709557994\n",
      "Epoch: 245\n",
      "3.6081845254321006\n",
      "Epoch: 246\n",
      "3.6062292347926075\n",
      "Epoch: 247\n",
      "3.6042997575319697\n",
      "Epoch: 248\n",
      "3.6023948610106813\n",
      "Epoch: 249\n",
      "3.600522214936826\n",
      "Epoch: 250\n",
      "3.5986969801237194\n",
      "Epoch: 251\n",
      "3.5969336940124395\n",
      "Epoch: 252\n",
      "3.5952370870544126\n",
      "Epoch: 253\n",
      "3.593599391030983\n",
      "Epoch: 254\n",
      "3.592003920015162\n",
      "Epoch: 255\n",
      "3.590428995003612\n",
      "Epoch: 256\n",
      "3.5888486338034897\n",
      "Epoch: 257\n",
      "3.5872331831304916\n",
      "Epoch: 258\n",
      "3.5855560177978627\n",
      "Epoch: 259\n",
      "3.583803249984541\n",
      "Epoch: 260\n",
      "3.5819752722819445\n",
      "Epoch: 261\n",
      "3.58007873918076\n",
      "Epoch: 262\n",
      "3.578120333420449\n",
      "Epoch: 263\n",
      "3.5761058447789438\n",
      "Epoch: 264\n",
      "3.574039121801597\n",
      "Epoch: 265\n",
      "3.5719218236572083\n",
      "Epoch: 266\n",
      "3.569756593868013\n",
      "Epoch: 267\n",
      "3.567551401144911\n",
      "Epoch: 268\n",
      "3.565320115286113\n",
      "Epoch: 269\n",
      "3.5630785903577507\n",
      "Epoch: 270\n",
      "3.5608403358017937\n",
      "Epoch: 271\n",
      "3.5586153064454695\n",
      "Epoch: 272\n",
      "3.556411504209991\n",
      "Epoch: 273\n",
      "3.5542362707229835\n",
      "Epoch: 274\n",
      "3.5520959915229535\n",
      "Epoch: 275\n",
      "3.5499950455460585\n",
      "Epoch: 276\n",
      "3.5479347104484615\n",
      "Epoch: 277\n",
      "3.5459133398896046\n",
      "Epoch: 278\n",
      "3.543928248652742\n",
      "Epoch: 279\n",
      "3.5419775787420646\n",
      "Epoch: 280\n",
      "3.540060920858416\n",
      "Epoch: 281\n",
      "3.5381790127530413\n",
      "Epoch: 282\n",
      "3.5363334648546485\n",
      "Epoch: 283\n",
      "3.5345268704143296\n",
      "Epoch: 284\n",
      "3.532762135009525\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 285\n",
      "3.531041029902553\n",
      "Epoch: 286\n",
      "3.5293634439409325\n",
      "Epoch: 287\n",
      "3.527728066461936\n",
      "Epoch: 288\n",
      "3.5261341534217094\n",
      "Epoch: 289\n",
      "3.524582693996962\n",
      "Epoch: 290\n",
      "3.5230762501499617\n",
      "Epoch: 291\n",
      "3.521618682694703\n",
      "Epoch: 292\n",
      "3.5202153290221783\n",
      "Epoch: 293\n",
      "3.518872119813997\n",
      "Epoch: 294\n",
      "3.5175924786769044\n",
      "Epoch: 295\n",
      "3.516373996338058\n",
      "Epoch: 296\n",
      "3.515208312461248\n",
      "Epoch: 297\n",
      "3.5140850197839706\n",
      "Epoch: 298\n",
      "3.51299734058765\n",
      "Epoch: 299\n",
      "3.5119459841884866\n",
      "Epoch: 300\n",
      "3.510938434400634\n",
      "Epoch: 301\n",
      "3.5099844504494437\n",
      "Epoch: 302\n",
      "3.509091444155227\n",
      "Epoch: 303\n",
      "3.508262492010876\n",
      "Epoch: 304\n",
      "3.507496700876427\n",
      "Epoch: 305\n",
      "3.506789829990442\n",
      "Epoch: 306\n",
      "3.5061343457869434\n",
      "Epoch: 307\n",
      "3.5055200041416685\n",
      "Epoch: 308\n",
      "3.5049354057279434\n",
      "Epoch: 309\n",
      "3.504369097145006\n",
      "Epoch: 310\n",
      "3.5038093179288996\n",
      "Epoch: 311\n",
      "3.5032439695203177\n",
      "Epoch: 312\n",
      "3.5026620799031862\n",
      "Epoch: 313\n",
      "3.502055038545824\n",
      "Epoch: 314\n",
      "3.501415829598776\n",
      "Epoch: 315\n",
      "3.5007379752723353\n",
      "Epoch: 316\n",
      "3.5000172021638503\n",
      "Epoch: 317\n",
      "3.49925359288745\n",
      "Epoch: 318\n",
      "3.4984505008818596\n",
      "Epoch: 319\n",
      "3.497612376432167\n",
      "Epoch: 320\n",
      "3.4967444119835998\n",
      "Epoch: 321\n",
      "3.495852964765026\n",
      "Epoch: 322\n",
      "3.4949457322680297\n",
      "Epoch: 323\n",
      "3.4940321183313854\n",
      "Epoch: 324\n",
      "3.493123533504885\n",
      "Epoch: 325\n",
      "3.49223225884663\n",
      "Epoch: 326\n",
      "3.4913674776966057\n",
      "Epoch: 327\n",
      "3.49053026341021\n",
      "Epoch: 328\n",
      "3.4897133947648524\n",
      "Epoch: 329\n",
      "3.4889071998568766\n",
      "Epoch: 330\n",
      "3.488105037675273\n",
      "Epoch: 331\n",
      "3.487304545704876\n",
      "Epoch: 332\n",
      "3.4865061416940684\n",
      "Epoch: 333\n",
      "3.4857118940586798\n",
      "Epoch: 334\n",
      "3.484926332641109\n",
      "Epoch: 335\n",
      "3.4841577061055546\n",
      "Epoch: 336\n",
      "3.483418131871009\n",
      "Epoch: 337\n",
      "3.482723222665995\n",
      "Epoch: 338\n",
      "3.482089898432745\n",
      "Epoch: 339\n",
      "3.4815308157077394\n",
      "Epoch: 340\n",
      "3.481050074098548\n",
      "Epoch: 341\n",
      "3.480642719945363\n",
      "Epoch: 342\n",
      "3.4802954437159257\n",
      "Epoch: 343\n",
      "3.4799878904429042\n",
      "Epoch: 344\n",
      "3.4796955754571295\n",
      "Epoch: 345\n",
      "3.4793933504542647\n",
      "Epoch: 346\n",
      "3.479058512538393\n",
      "Epoch: 347\n",
      "3.478673878915414\n",
      "Epoch: 348\n",
      "3.478229787502918\n",
      "Epoch: 349\n",
      "3.477723495796788\n",
      "Epoch: 350\n",
      "3.4771563424745735\n",
      "Epoch: 351\n",
      "3.476530840956329\n",
      "Epoch: 352\n",
      "3.4758499037144346\n",
      "Epoch: 353\n",
      "3.475118335372477\n",
      "Epoch: 354\n",
      "3.474344859761524\n",
      "Epoch: 355\n",
      "3.4735437068973782\n",
      "Epoch: 356\n",
      "3.4727343312981738\n",
      "Epoch: 357\n",
      "3.471938171109271\n",
      "Epoch: 358\n",
      "3.4711741581172046\n",
      "Epoch: 359\n",
      "3.470455119292833\n",
      "Epoch: 360\n",
      "3.4697856796438504\n",
      "Epoch: 361\n",
      "3.4691616554959697\n",
      "Epoch: 362\n",
      "3.468571131153432\n",
      "Epoch: 363\n",
      "3.467997461723693\n",
      "Epoch: 364\n",
      "3.4674236404593812\n",
      "Epoch: 365\n",
      "3.4668366040744254\n",
      "Epoch: 366\n",
      "3.4662304026965454\n",
      "Epoch: 367\n",
      "3.465607637702973\n",
      "Epoch: 368\n",
      "3.464977988234013\n",
      "Epoch: 369\n",
      "3.4643532025990913\n",
      "Epoch: 370\n",
      "3.4637406495743384\n",
      "Epoch: 371\n",
      "3.463139647697016\n",
      "Epoch: 372\n",
      "3.4625433940780757\n",
      "Epoch: 373\n",
      "3.4619446161398932\n",
      "Epoch: 374\n",
      "3.4613403282429784\n",
      "Epoch: 375\n",
      "3.4607327974308912\n",
      "Epoch: 376\n",
      "3.460126565989825\n",
      "Epoch: 377\n",
      "3.4595244642738905\n",
      "Epoch: 378\n",
      "3.458927437409638\n",
      "Epoch: 379\n",
      "3.458338310606243\n",
      "Epoch: 380\n",
      "3.457762826345451\n",
      "Epoch: 381\n",
      "3.4572043117939426\n",
      "Epoch: 382\n",
      "3.456659643940748\n",
      "Epoch: 383\n",
      "3.4561241537702614\n",
      "Epoch: 384\n",
      "3.455596339319721\n",
      "Epoch: 462\n",
      "3.436616591234567\n",
      "Epoch: 463\n",
      "3.4363574764294778\n",
      "Epoch: 464\n",
      "3.4360788938730606\n",
      "Epoch: 465\n",
      "3.43578039692668\n",
      "Epoch: 466\n",
      "3.4354674851855442\n",
      "Epoch: 467\n",
      "3.4351504020070682\n",
      "Epoch: 468\n",
      "3.4348397919165956\n",
      "Epoch: 469\n",
      "3.4345433982830826\n",
      "Epoch: 470\n",
      "3.434265772960235\n",
      "Epoch: 471\n",
      "3.4340083337601075\n",
      "Epoch: 472\n",
      "3.4337677632434334\n",
      "Epoch: 473\n",
      "3.4335352459699533\n",
      "Epoch: 474\n",
      "3.4332995734127483\n",
      "Epoch: 475\n",
      "3.4330535229565458\n",
      "Epoch: 476\n",
      "3.432798069897302\n",
      "Epoch: 477\n",
      "3.432540986924105\n",
      "Epoch: 478\n",
      "3.4322925823605184\n",
      "Epoch: 479\n",
      "3.4320622148484197\n",
      "Epoch: 480\n",
      "3.431856876559231\n",
      "Epoch: 481\n",
      "3.431680970987936\n",
      "Epoch: 482\n",
      "3.4315352870372813\n",
      "Epoch: 483\n",
      "3.4314146335806903\n",
      "Epoch: 484\n",
      "3.431308215595498\n",
      "Epoch: 485\n",
      "3.431204903038418\n",
      "Epoch: 486\n",
      "3.4310984379949767\n",
      "Epoch: 487\n",
      "3.430987607322405\n",
      "Epoch: 488\n",
      "3.4308726997107963\n",
      "Epoch: 489\n",
      "3.430752037186461\n",
      "Epoch: 490\n",
      "3.4306200728971454\n",
      "Epoch: 491\n",
      "3.4304671717440938\n",
      "Epoch: 492\n",
      "3.4302811291330952\n",
      "Epoch: 493\n",
      "3.430049773848403\n",
      "Epoch: 494\n",
      "3.429763958731823\n",
      "Epoch: 495\n",
      "3.4294203393277636\n",
      "Epoch: 496\n",
      "3.429022276476124\n",
      "Epoch: 497\n",
      "3.428577949429697\n",
      "Epoch: 498\n",
      "3.428097169881399\n",
      "Epoch: 499\n",
      "3.4275886253163885\n",
      "Epoch: 500\n",
      "3.4270571746606637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Q2 starts here!\n",
    "\n",
    "# x_train = x_train.reshape((num_examples, 10000))\n",
    "# y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "\n",
    "# activation function and its derivative\n",
    "def sigmoid(x, deriv=False):\n",
    "    if deriv:\n",
    "        return sigmoid(x) * (1 - sigmoid(x))\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    exps = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return exps/np.sum(exps, axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "class Layer:\n",
    "    # initialze layers with weights, biases, cache and deltas arrays\n",
    "    def __init__(self, num_input, num_output):\n",
    "        self.weights = np.random.randn(num_input, num_output)\n",
    "        self.biases = np.zeros((1, num_output))\n",
    "        self.cache = None\n",
    "        self.deltas = None\n",
    "    \n",
    "    # activation function for all layers except the last\n",
    "    def activation(self, inputs):\n",
    "        self.cache = sigmoid(np.dot(inputs, self.weights) + self.biases)\n",
    "        return self.cache\n",
    "    # activation function for the output layer\n",
    "    def activation_last(self, inputs):\n",
    "        self.cache = softmax(np.dot(inputs, self.weights) + self.biases)\n",
    "        return self.cache\n",
    "\n",
    "\n",
    "class NeuralNetwork:\n",
    "\n",
    "    def __init__(self, x_train, y_train, dimentions, epochs, learning_rate):\n",
    "        # initialize nn model\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.losses = []\n",
    "\n",
    "        self.layers = []\n",
    "        for i in range(len(dimentions) - 1):\n",
    "            self.layers.append(Layer(dimentions[i], dimentions[i + 1]))\n",
    "\n",
    "    # cross entropy function\n",
    "    def cross_entropy(self, pred):\n",
    "        return (pred - self.y_train) / self.y_train.shape[0]\n",
    "    \n",
    "    # calculate loss\n",
    "    def loss(self, pred):\n",
    "        n = self.y_train.shape[0]\n",
    "        logp = - np.log(pred[np.arange(n), self.y_train.argmax(axis=1)])\n",
    "        loss = np.sum(logp) / n\n",
    "        return loss\n",
    "\n",
    "    # feed forward process\n",
    "    def forward(self, x):\n",
    "        # apply sigmoid function to all layers except the last layer\n",
    "        cache = self.layers[0].activation(x)\n",
    "        for layer in self.layers[1:-1]:\n",
    "            cache = layer.activation(cache)\n",
    "        # apply softmax function as the activation function for the output layer\n",
    "        cache = self.layers[-1].activation_last(cache)    \n",
    "        return cache\n",
    "\n",
    "    # backpropagation process\n",
    "    def backprop(self, x):\n",
    "        # compute loss for each epoch\n",
    "        loss = self.loss(self.layers[-1].cache)\n",
    "        self.losses.append(loss)\n",
    "        print(loss)\n",
    "        # layer deltas are computed using the derivative of the activation function\n",
    "        # cross entropy function is used here as the derivative of softmax function is cancelled out in chain rule\n",
    "        self.layers[-1].deltas = self.cross_entropy(self.layers[-1].cache)\n",
    "        z_delta = np.dot(self.layers[-1].deltas, self.layers[-1].weights.T)\n",
    "        # z delta of a backward layer is used to compute the deltas of a forward layer\n",
    "        for layer in self.layers[-2:0:-1]:\n",
    "            layer.deltas = z_delta * sigmoid(layer.cache, deriv=True)\n",
    "            z_delta = np.dot(layer.deltas, layer.weights.T)\n",
    "\n",
    "        self.layers[0].deltas = z_delta * sigmoid(self.layers[0].cache, deriv=True)\n",
    "\n",
    "        # update weights and biases using gradient descent approach in all layers\n",
    "        self.layers[0].weights -= self.learning_rate * np.dot(x.T, self.layers[0].deltas)\n",
    "        self.layers[0].biases -= self.learning_rate * np.sum(self.layers[0].deltas, axis=0)\n",
    "        cache = self.layers[0].cache\n",
    "\n",
    "        for layer in self.layers[1:]:\n",
    "            layer.weights -= self.learning_rate * np.dot(cache.T, layer.deltas)\n",
    "            layer.biases -= self.learning_rate * np.sum(layer.deltas, axis=0)\n",
    "            cache = layer.cache\n",
    "\n",
    "    def predict(self, x):\n",
    "        return np.apply_along_axis(np.argmax, 1, self.forward(x))\n",
    "\n",
    "    # fit the model\n",
    "    def fit(self):\n",
    "        for i in range(1, self.epochs + 1):\n",
    "            print(\"Epoch: {}\".format(i))\n",
    "            self.forward(self.x_train)\n",
    "            self.backprop(self.x_train)\n",
    "    \n",
    "    # plot loss graph\n",
    "    def plot(self):\n",
    "        plt.figure()\n",
    "        plt.xlabel(\"epoch\")\n",
    "        plt.ylabel(\"loss\")\n",
    "        plt.plot(range(1, self.epochs + 1), self.losses)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n",
    "model = NeuralNetwork(x_train, y_train, [10000, 800, 400, 100, 31], 500, 0.1)\n",
    "model.fit()\n",
    "model.plot()\n",
    "\n",
    "\n",
    "\n",
    "# accuracy\n",
    "\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# print(accuracy_score(y_raw.astype(int), model.predict(x_train)))\n",
    "# print(accuracy_score(y_valid_raw.astype(int), model.predict(x_valid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy:\n",
      "0.08644444444444445\n",
      "Validation accuracy:\n",
      "0.065\n"
     ]
    }
   ],
   "source": [
    "# Calculate training and valid set accuracy\n",
    "\n",
    "y_train = np.apply_along_axis(np.argmax, 1, y_train)\n",
    "y_validation = np.apply_along_axis(np.argmax, 1, y_validation)\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Train accuracy:\")\n",
    "print(accuracy_score(y_train, model.predict(x_train)))\n",
    "print(\"Validation accuracy:\")\n",
    "print(accuracy_score(y_validation, model.predict(x_validation)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
